{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from torchsummaryX import summary\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09010f30",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8356dbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.global_exception_handler.v1.GlobalExceptionHandler at 0x7f939412bee0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from setup import get_package_root_path\n",
    "from src.global_exception_handler.v1 import GlobalExceptionHandler\n",
    "from src.webhook.v1 import TeamsWebhook\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pakage_name = os.environ.get(\"PACKAGE_NAME\")\n",
    "root_path = get_package_root_path()\n",
    "\n",
    "# 웹훅 알림 url (없으면 빈 문자열)\n",
    "webhook_url = os.environ.get(\"WEBHOOK_URL\")\n",
    "webhook = TeamsWebhook(webhook_url)\n",
    "\n",
    "# 핸들링할 예외 종류\n",
    "except_tuple = (Exception,)\n",
    "GlobalExceptionHandler(except_tuple=except_tuple, sender=webhook, name=\"dacon_cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09bb8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 14 15:00:25 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:81:00.0 Off |                  Off |\n",
      "| 35%   55C    P0    89W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              'root_path': '/data/dacon_cars',\n",
       "              'job_id': 'DETECTION_2',\n",
       "              'data_path': '/data/dacon_cars/data',\n",
       "              'outputs_path': '/data/dacon_cars/outputs/DETECTION_2',\n",
       "              'predict_dir': '/data/dacon_cars/outputs/DETECTION_2/predict',\n",
       "              'recorder_dir': '/data/dacon_cars/outputs/DETECTION_2/recorder',\n",
       "              'learning_late': 0.0001,\n",
       "              'batch_size': 8,\n",
       "              'epoch': 10,\n",
       "              '__dict__': <attribute '__dict__' of 'CFG' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'CFG' objects>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    root_path = root_path\n",
    "    # Job Id (보통 파일명과 동일하게)\n",
    "    job_id = \"DETECTION_2\"\n",
    "\n",
    "    # 원천 데이터 경로\n",
    "    data_path = f\"{root_path}/data\"\n",
    "\n",
    "    # 학습의 결과물이 저장될 경로\n",
    "    outputs_path = f\"{root_path}/outputs/{job_id}\"\n",
    "    predict_dir = f\"{outputs_path}/predict\"\n",
    "    recorder_dir = f\"{outputs_path}/recorder\"\n",
    "\n",
    "    learning_late = 0.0001\n",
    "    batch_size = 8\n",
    "    epoch = 10\n",
    "\n",
    "\n",
    "CFG.__dict__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16fd60a5-24e2-4539-bfd0-1c374a641699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    def __init__(self, json_path, data_path, transforms=None):\n",
    "        self.coco = COCO(json_path)\n",
    "        self.image_ids = list(self.coco.imgToAnns.keys())\n",
    "        self.transforms = transforms\n",
    "        self.data_path = data_path\n",
    "        self.classes = []\n",
    "        for v in self.coco.cats.values():\n",
    "            self.classes.append(v[\"name\"])\n",
    "\n",
    "        # 어노테이션 정보는 있지만 이미지 파일은 없는 데이터 제거 시작\n",
    "        temp_image_ids = []\n",
    "        image_files = os.listdir(f\"{data_path}\")\n",
    "        for image_id in self.image_ids:\n",
    "            if self.coco.loadImgs(image_id)[0][\"file_name\"] in image_files:\n",
    "                temp_image_ids.append(image_id)\n",
    "        self.image_ids = temp_image_ids\n",
    "        # 어노테이션 정보는 있지만 이미지 파일은 없는 데이터 제거 끝\n",
    "\n",
    "    def get_mean_std(self):\n",
    "        img_norm = list()\n",
    "        img_std = list()\n",
    "        for image_id in self.image_ids:\n",
    "            file_name = self.coco.loadImgs(image_id)[0][\"file_name\"]\n",
    "            file_name = os.path.join(self.data_path, file_name)\n",
    "            img = cv2.imread(file_name, cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "            if len(img.shape) < 2:  # 흑백 이미지는 제외\n",
    "                continue\n",
    "            mean, std = np.mean(img, axis=(0, 1)), np.std(img, axis=(0, 1))\n",
    "            img_norm.append(mean)\n",
    "            img_std.append(std)\n",
    "\n",
    "        print(np.mean(img_norm, axis=0), np.mean(img_std, axis=0))\n",
    "\n",
    "    def get_categories(self):\n",
    "        return self.coco.cats\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        file_name = self.coco.loadImgs(image_id)[0][\"file_name\"]\n",
    "        path = os.path.join(self.data_path, file_name)\n",
    "        image = cv2.imread(path)\n",
    "\n",
    "        annot_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        annots = [x for x in self.coco.loadAnns(annot_ids) if x[\"image_id\"] == image_id]\n",
    "\n",
    "        boxes = np.array([annot[\"bbox\"] for annot in annots], dtype=np.float32)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "        labels = np.array([annot[\"category_id\"] for annot in annots], dtype=np.int32)\n",
    "\n",
    "        area = np.array([annot[\"area\"] for annot in annots], dtype=np.float32)\n",
    "        iscrowd = np.array([annot[\"iscrowd\"] for annot in annots], dtype=np.uint8)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(\n",
    "                image=image,\n",
    "                bboxes=boxes,\n",
    "                category_ids=labels,\n",
    "            )\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": transformed[\"bboxes\"],\n",
    "            \"labels\": transformed[\"category_ids\"],\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd,\n",
    "        }\n",
    "\n",
    "        target[\"boxes\"] = torch.as_tensor(target[\"boxes\"], dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(target[\"labels\"], dtype=torch.int64)\n",
    "        target[\"area\"] = torch.as_tensor(target[\"area\"], dtype=torch.float32)\n",
    "        target[\"iscrowd\"] = torch.as_tensor(target[\"iscrowd\"], dtype=torch.uint8)\n",
    "\n",
    "        return transformed[\"image\"], target, path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42a7a17f",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01b4067-0669-44a9-bbbc-3065d8cb00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.my_albumentations.v1 import CustomBBoxSafeRandomCrop\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(1080, 1920, border_mode=0, value=(0, 0, 0)),\n",
    "        A.ShiftScaleRotate(\n",
    "            scale_limit=0.01,\n",
    "            rotate_limit=5,\n",
    "            border_mode=0,\n",
    "            value=(0, 0, 0),\n",
    "            p=1,\n",
    "        ),\n",
    "        A.HorizontalFlip(),\n",
    "        A.ToGray(p=1),\n",
    "        A.Equalize(by_channels=False),\n",
    "        A.Downscale(\n",
    "            interpolation=2,\n",
    "        ),\n",
    "        # A.ElasticTransform(\n",
    "        #     border_mode=0,\n",
    "        #     value=(0, 0, 0),\n",
    "        # ),\n",
    "        A.GaussNoise(p=0.9),\n",
    "        A.HueSaturationValue(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=1),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"category_ids\"]),\n",
    ")\n",
    "\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(1080, 1920, border_mode=0, value=(0, 0, 0)),\n",
    "        A.ToGray(p=1),\n",
    "        A.Equalize(by_channels=False),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"category_ids\"]),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed83ca14",
   "metadata": {},
   "source": [
    "## 파일 랜덤 fold 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6a223cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from src.image_eda.v1 import divide_list_evenly\n",
    "\n",
    "if not os.path.exists(f\"{CFG.data_path}/train_0\"):\n",
    "    data_path_list = sorted(glob.glob(f\"{CFG.data_path}/train/*.png\"))\n",
    "\n",
    "    print(len(data_path_list))\n",
    "\n",
    "    # split_index = int(np.round(len(data_path_list) * 0.8))\n",
    "    random.shuffle(data_path_list)\n",
    "    list_chunked = divide_list_evenly(data_path_list, 5)\n",
    "\n",
    "    fold_count = 5\n",
    "    for fold_index in range(fold_count):\n",
    "        os.makedirs(f\"{CFG.data_path}/train_{fold_index}\", exist_ok=True)\n",
    "        os.makedirs(f\"{CFG.data_path}/val_{fold_index}\", exist_ok=True)\n",
    "\n",
    "        train_image_list = []\n",
    "        for i in range(fold_count):\n",
    "            if i == fold_index:\n",
    "                continue\n",
    "            train_image_list += list_chunked[i]\n",
    "        print(len(train_image_list))\n",
    "        for data_path in train_image_list:\n",
    "            shutil.copy(data_path, data_path.replace(\"train\", f\"train_{fold_index}\"))\n",
    "\n",
    "        val_image_list = list_chunked[fold_index]\n",
    "        print(len(val_image_list))\n",
    "        for data_path in val_image_list:\n",
    "            shutil.copy(data_path, data_path.replace(\"train\", f\"val_{fold_index}\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "## Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1011ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.image_eda.v1 import tensor2im, apply_bbox, put_text\n",
    "from src.rcnn_utils.v1 import eval_forward\n",
    "\n",
    "\n",
    "def valid(model, data_loader, device):\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (images, targets, paths) in enumerate(tqdm(data_loader)):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            losses_dict, detections = eval_forward(model, images, targets)\n",
    "\n",
    "            if batch_index % 10 == 0:\n",
    "                temp_img = images[0].detach().cpu()\n",
    "                temp_img = tensor2im(temp_img)\n",
    "                target = targets[0].copy()\n",
    "                for k, v in target.items():\n",
    "                    target[k] = target[k].detach().cpu().int().numpy()\n",
    "\n",
    "                boxes = detections[0][\"boxes\"].detach().cpu().int().numpy()  #\n",
    "                labels = detections[0][\"labels\"].detach().cpu().int().numpy()  #\n",
    "                scores = detections[0][\"scores\"].detach().cpu().tolist()  #\n",
    "\n",
    "                thresholded_preds_inidices = [\n",
    "                    scores.index(i) for i in scores if i > 0.7\n",
    "                ]\n",
    "                thresholded_preds_count = len(thresholded_preds_inidices)\n",
    "\n",
    "                if thresholded_preds_count > 0:\n",
    "                    scores = scores[:thresholded_preds_count]\n",
    "                    labels = labels[:thresholded_preds_count]\n",
    "                    boxes = boxes[:thresholded_preds_count]\n",
    "                    boxes = boxes.astype(np.int32).tolist()\n",
    "\n",
    "                    temp_img = apply_bbox(\n",
    "                        temp_img,\n",
    "                        labels,\n",
    "                        boxes,\n",
    "                        data_loader.dataset.get_classes(),\n",
    "                        scores,\n",
    "                    )\n",
    "\n",
    "                temp_img = put_text(\n",
    "                    temp_img, os.path.basename(paths[0]), (0, 0), [0, 0, 255]\n",
    "                )\n",
    "                cv2.imwrite(f\"{CFG.root_path}/temp/valid_img.jpg\", temp_img)\n",
    "\n",
    "            losses = sum(loss for loss in losses_dict.values())\n",
    "\n",
    "            val_loss.append(losses.item())\n",
    "\n",
    "    validation_loss = np.mean(val_loss)\n",
    "    return validation_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_loader, device, grad_scaler=None):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch_index, (images, targets, paths) in enumerate(tqdm(data_loader)):\n",
    "        if batch_index % 10 == 0:\n",
    "            temp_img = images[0].detach().cpu()\n",
    "            temp_img = tensor2im(temp_img)\n",
    "            target = targets[0].copy()\n",
    "            for k, v in target.items():\n",
    "                target[k] = target[k].int().numpy()\n",
    "\n",
    "            temp_img = apply_bbox(\n",
    "                temp_img,\n",
    "                target[\"labels\"],\n",
    "                target[\"boxes\"],\n",
    "                data_loader.dataset.get_classes(),\n",
    "            )\n",
    "\n",
    "            temp_img = put_text(\n",
    "                temp_img, os.path.basename(paths[0]), (0, 0), [0, 0, 255]\n",
    "            )\n",
    "            cv2.imwrite(f\"{CFG.root_path}/temp/train_img.jpg\", temp_img)\n",
    "\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if grad_scaler is None:\n",
    "            losses = model(images, targets)\n",
    "            loss: torch.Tensor = sum(loss for loss in losses.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                losses = model(images, targets)\n",
    "                loss: torch.Tensor = sum(loss for loss in losses.values())\n",
    "\n",
    "            grad_scaler.scale(loss).backward()\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    _train_loss = np.mean(train_loss)\n",
    "    return _train_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcd63b0f",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0caf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rcnn_utils.v1 import CustomRCNNTransform\n",
    "from torchvision.models.detection.faster_rcnn import (\n",
    "    FastRCNNPredictor,\n",
    "    FasterRCNN,\n",
    "    AnchorGenerator,\n",
    ")\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "\n",
    "def create_model(num_classes: int):\n",
    "    backbone = resnet_fpn_backbone(\n",
    "        backbone_name=\"resnext50_32x4d\",\n",
    "        weights=models.ResNeXt50_32X4D_Weights.DEFAULT,\n",
    "        trainable_layers=5,\n",
    "    )\n",
    "\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=(\n",
    "            (230,),\n",
    "            (270,),\n",
    "            (310,),\n",
    "            (350,),\n",
    "            (390,),\n",
    "        ),\n",
    "        aspect_ratios=(\n",
    "            (0.8, 0.9, 1, 1.1),\n",
    "            (0.8, 0.9, 1, 1.1),\n",
    "            (0.8, 0.9, 1, 1.1),\n",
    "            (0.8, 0.9, 1, 1.1),\n",
    "            (0.8, 0.9, 1, 1.1),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        num_classes=num_classes,\n",
    "        rpn_fg_iou_thresh=0.85,\n",
    "        rpn_bg_iou_thresh=0.4,\n",
    "        # rpn_positive_fraction=0.5,\n",
    "        box_fg_iou_thresh=0.85,\n",
    "        box_bg_iou_thresh=0.4,\n",
    "        # box_positive_fraction=0.5,\n",
    "    )\n",
    "\n",
    "    model.transform = CustomRCNNTransform()\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b230af0",
   "metadata": {},
   "source": [
    "## Snapshot Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d6e487f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/dacon_cars/outputs/DETECTION_2/DETECTION_2.ipynb'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "try:\n",
    "    import IPython\n",
    "\n",
    "    notebook_path = IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"]\n",
    "except:\n",
    "    notebook_path = f\"{os.getcwd()}/{CFG.job_id}.ipynb\"\n",
    "\n",
    "\n",
    "os.makedirs(CFG.outputs_path, exist_ok=True)\n",
    "shutil.copy(notebook_path, f\"{CFG.outputs_path}/{os.path.split(notebook_path)[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005b7ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6358e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.random_seed.v1 import seed_everything, seed_worker\n",
    "\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc06262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간이 더 걸림\n",
    "grad_scaler = None  # torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86142d9a-68b7-4d04-8423-49d28025411d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "fold_0 start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 102/648 [26:18<2:20:52, 15.48s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m seed_everything(epoch_index)\n\u001b[1;32m     58\u001b[0m train_start_timestamp \u001b[39m=\u001b[39m time()\n\u001b[0;32m---> 59\u001b[0m train_loss \u001b[39m=\u001b[39m train(model, optimizer, train_loader, device, grad_scaler)\n\u001b[1;32m     60\u001b[0m train_elapsed_time \u001b[39m=\u001b[39m time() \u001b[39m-\u001b[39m train_start_timestamp\n\u001b[1;32m     62\u001b[0m val_start_timestamp \u001b[39m=\u001b[39m time()\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, data_loader, device, grad_scaler)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m train_loss \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m batch_index, (images, targets, paths) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(data_loader)):\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m batch_index \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      7\u001b[0m         temp_img \u001b[39m=\u001b[39m images[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1285\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    180\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    307\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.recorder.v1 import Recorder\n",
    "from time import time\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "for fold_index in range(5):\n",
    "    train_dataset = COCODataset(\n",
    "        f\"{CFG.data_path}/annotations/train.json\",\n",
    "        f\"{CFG.data_path}/train_{fold_index}\",\n",
    "        train_transform,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=16,  #\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        worker_init_fn=seed_worker,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    #\n",
    "    val_dataset = COCODataset(\n",
    "        f\"{CFG.data_path}/annotations/train.json\",\n",
    "        f\"{CFG.data_path}/val_{fold_index}\",\n",
    "        val_transform,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=16,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        worker_init_fn=seed_worker,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    model = create_model(len(train_dataset.get_categories()) + 1)\n",
    "    model.to(device)\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=CFG.learning_late)\n",
    "    scheduler = None\n",
    "\n",
    "    recorder = Recorder(\n",
    "        f\"{CFG.recorder_dir}/fold_{fold_index}\", model, optimizer, scheduler\n",
    "    )\n",
    "    print(f\"fold_{fold_index} start\")\n",
    "    if recorder.load_checkpoint(device, \"checkpoint.pt\"):\n",
    "        print(f\"loaded current_epoch: {recorder.current_epoch}\")\n",
    "\n",
    "    best_val_loss = 100\n",
    "    for epoch_index in range(recorder.current_epoch, CFG.epoch):\n",
    "        seed_everything(epoch_index)\n",
    "\n",
    "        train_start_timestamp = time()\n",
    "        train_loss = train(model, optimizer, train_loader, device, grad_scaler)\n",
    "        train_elapsed_time = time() - train_start_timestamp\n",
    "\n",
    "        val_start_timestamp = time()\n",
    "        val_loss = valid(model, val_loader, device)\n",
    "        val_elapsed_time = time() - val_start_timestamp\n",
    "\n",
    "        recorder.update_row_dict(\"epoch\", epoch_index + 1)\n",
    "        recorder.update_row_dict(\"train_loss\", train_loss)\n",
    "        recorder.update_row_dict(\"val_loss\", val_loss)\n",
    "        recorder.update_row_dict(\"train_elapsed_time\", train_elapsed_time)\n",
    "        recorder.update_row_dict(\"val_elapsed_time\", val_elapsed_time)\n",
    "        recorder.flush_row_dict(is_print=True)\n",
    "        recorder.save_line_plot([\"loss\"], [0, 0.1])\n",
    "\n",
    "        if recorder.is_best_score(val_loss, \"min\"):\n",
    "            print(f\"best epoch: {epoch_index + 1}\")\n",
    "            recorder.save_checkpoint(epoch_index, \"best_model.pt\")\n",
    "\n",
    "        recorder.save_checkpoint(epoch_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e86e53d",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(1080, 1920, border_mode=0, value=(0, 0, 0)),\n",
    "        A.ToGray(p=1),\n",
    "        A.Equalize(by_channels=False),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferencePatchDataset(Dataset):\n",
    "    def __init__(self, X_list, transforms):\n",
    "        self.X_list = X_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X_list)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img, x, y = self.X_list[index]\n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        return img, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img, x, y, w, h):\n",
    "    if y + h <= img.shape[0] and x + w <= img.shape[1]:\n",
    "        # 이미지 범위 안에 들어오는 사이즈\n",
    "        return img[y : y + h, x : x + w], y, x\n",
    "\n",
    "    result_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    # 이미지 사이즈를 넘어가는 crop의 경우\n",
    "    if y + h > img.shape[0] and x + w > img.shape[1]:\n",
    "        temp_img = img[y:, x:]\n",
    "    elif y + h > img.shape[0]:\n",
    "        temp_img = img[y:, x : x + w]\n",
    "    elif x + w > img.shape[1]:\n",
    "        temp_img = img[y : y + h, x:]\n",
    "    else:\n",
    "        raise Exception(\"crop error\")\n",
    "\n",
    "    # zero padding\n",
    "    result_img[: temp_img.shape[0], : temp_img.shape[1]] = temp_img\n",
    "\n",
    "    return result_img, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa4e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset = COCODataset(\n",
    "    f\"{CFG.data_path}/annotations/train.json\",\n",
    "    f\"{CFG.data_path}/train_0\",\n",
    "    train_transform,\n",
    ")\n",
    "\n",
    "\n",
    "num_classes = len(temp_dataset.get_categories()) + 1\n",
    "classes = temp_dataset.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_classes)\n",
    "model.cuda()\n",
    "model.to(device)\n",
    "\n",
    "check_point = torch.load(\n",
    "    f\"{CFG.recorder_dir}/fold_0/checkpoint.pt\",\n",
    "    map_location=device,\n",
    ")\n",
    "model.load_state_dict(check_point[\"model\"])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada847c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.my_nms.v1 import non_max_suppression\n",
    "\n",
    "test_image_path = f\"{CFG.data_path}/test\"\n",
    "test_image_list = sorted(glob.glob(f\"{test_image_path}/*.png\"))\n",
    "\n",
    "result_save_path = f\"{CFG.predict_dir}\"\n",
    "os.makedirs(result_save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for test_image_path in test_image_list:\n",
    "    print(test_image_path)\n",
    "    img = cv2.imread(test_image_path)\n",
    "    img = test_transform(image=img)[\"image\"]\n",
    "    img = img.to(device)\n",
    "    print(img.shape)\n",
    "    print(img)\n",
    "    result = model(img)\n",
    "\n",
    "    boxes = result[\"boxes\"].detach().cpu().int().numpy()\n",
    "    labels = result[\"labels\"].detach().cpu().int().numpy()\n",
    "    scores = result[\"scores\"].detach().cpu().tolist()\n",
    "\n",
    "    boxes = boxes[0]\n",
    "    labels = labels[0]\n",
    "    scores = scores[0]\n",
    "\n",
    "    test_img = apply_bbox(\n",
    "        test_img,\n",
    "        labels,\n",
    "        boxes,\n",
    "        classes,\n",
    "        scores,\n",
    "        [0, 0, 255],\n",
    "    )\n",
    "\n",
    "    cv2.imwrite(\n",
    "        f\"{result_save_path}/{os.path.basename(test_image_path).replace('png', 'jpg')}\",\n",
    "        test_img,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2af0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096656f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
