{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from torchsummaryX import summary\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09010f30",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8356dbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.global_exception_handler.v1.GlobalExceptionHandler at 0x7f1cf43393d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from setup import get_package_root_path\n",
    "from src.global_exception_handler.v1 import GlobalExceptionHandler\n",
    "from src.webhook.v1 import TeamsWebhook\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pakage_name = os.environ.get(\"PACKAGE_NAME\")\n",
    "root_path = get_package_root_path()\n",
    "\n",
    "# 웹훅 알림 url (없으면 빈 문자열)\n",
    "webhook_url = os.environ.get(\"WEBHOOK_URL\")\n",
    "webhook = TeamsWebhook(webhook_url)\n",
    "\n",
    "# 핸들링할 예외 종류\n",
    "except_tuple = (Exception,)\n",
    "GlobalExceptionHandler(except_tuple=except_tuple, sender=webhook, name=\"dacon_cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09bb8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 11 07:23:19 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:81:00.0 Off |                  Off |\n",
      "| 30%   42C    P8    20W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              'root_path': '/data/dacon_cars',\n",
       "              'job_id': 'CLASSICIATION_1',\n",
       "              'data_path': '/data/dacon_cars/data/cars',\n",
       "              'outputs_path': '/data/dacon_cars/outputs/CLASSICIATION_1',\n",
       "              'predict_dir': '/data/dacon_cars/outputs/CLASSICIATION_1/predict',\n",
       "              'recorder_dir': '/data/dacon_cars/outputs/CLASSICIATION_1/recorder',\n",
       "              'learning_late': 0.0001,\n",
       "              'batch_size': 16,\n",
       "              'epoch': 7,\n",
       "              'num_classes': 34,\n",
       "              'classes': ['chevrolet_malibu_sedan_2012_2016',\n",
       "               'chevrolet_malibu_sedan_2017_2019',\n",
       "               'chevrolet_spark_hatchback_2016_2021',\n",
       "               'chevrolet_trailblazer_suv_2021_',\n",
       "               'chevrolet_trax_suv_2017_2019',\n",
       "               'genesis_g80_sedan_2016_2020',\n",
       "               'genesis_g80_sedan_2021_',\n",
       "               'genesis_gv80_suv_2020_',\n",
       "               'hyundai_avante_sedan_2011_2015',\n",
       "               'hyundai_avante_sedan_2020_',\n",
       "               'hyundai_grandeur_sedan_2011_2016',\n",
       "               'hyundai_grandstarex_van_2018_2020',\n",
       "               'hyundai_ioniq_hatchback_2016_2019',\n",
       "               'hyundai_sonata_sedan_2004_2009',\n",
       "               'hyundai_sonata_sedan_2010_2014',\n",
       "               'hyundai_sonata_sedan_2019_2020',\n",
       "               'kia_carnival_van_2015_2020',\n",
       "               'kia_carnival_van_2021_',\n",
       "               'kia_k5_sedan_2010_2015',\n",
       "               'kia_k5_sedan_2020_',\n",
       "               'kia_k7_sedan_2016_2020',\n",
       "               'kia_mohave_suv_2020_',\n",
       "               'kia_morning_hatchback_2004_2010',\n",
       "               'kia_morning_hatchback_2011_2016',\n",
       "               'kia_ray_hatchback_2012_2017',\n",
       "               'kia_sorrento_suv_2015_2019',\n",
       "               'kia_sorrento_suv_2020_',\n",
       "               'kia_soul_suv_2014_2018',\n",
       "               'kia_sportage_suv_2016_2020',\n",
       "               'kia_stonic_suv_2017_2019',\n",
       "               'renault_sm3_sedan_2015_2018',\n",
       "               'renault_xm3_suv_2020_',\n",
       "               'ssangyong_korando_suv_2019_2020',\n",
       "               'ssangyong_tivoli_suv_2016_2020'],\n",
       "              '__dict__': <attribute '__dict__' of 'CFG' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'CFG' objects>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    root_path = root_path\n",
    "    # Job Id (보통 파일명과 동일하게)\n",
    "    job_id = \"CLASSICIATION_1\"\n",
    "\n",
    "    # 원천 데이터 경로\n",
    "    data_path = f\"{root_path}/data/cars\"\n",
    "\n",
    "    # 학습의 결과물이 저장될 경로\n",
    "    outputs_path = f\"{root_path}/outputs/{job_id}\"\n",
    "    predict_dir = f\"{outputs_path}/predict\"\n",
    "    recorder_dir = f\"{outputs_path}/recorder\"\n",
    "\n",
    "    learning_late = 0.0001\n",
    "    batch_size = 16\n",
    "    epoch = 7\n",
    "    num_classes = 34\n",
    "\n",
    "    classes = [\n",
    "        \"chevrolet_malibu_sedan_2012_2016\",\n",
    "        \"chevrolet_malibu_sedan_2017_2019\",\n",
    "        \"chevrolet_spark_hatchback_2016_2021\",\n",
    "        \"chevrolet_trailblazer_suv_2021_\",\n",
    "        \"chevrolet_trax_suv_2017_2019\",\n",
    "        \"genesis_g80_sedan_2016_2020\",\n",
    "        \"genesis_g80_sedan_2021_\",\n",
    "        \"genesis_gv80_suv_2020_\",\n",
    "        \"hyundai_avante_sedan_2011_2015\",\n",
    "        \"hyundai_avante_sedan_2020_\",\n",
    "        \"hyundai_grandeur_sedan_2011_2016\",\n",
    "        \"hyundai_grandstarex_van_2018_2020\",\n",
    "        \"hyundai_ioniq_hatchback_2016_2019\",\n",
    "        \"hyundai_sonata_sedan_2004_2009\",\n",
    "        \"hyundai_sonata_sedan_2010_2014\",\n",
    "        \"hyundai_sonata_sedan_2019_2020\",\n",
    "        \"kia_carnival_van_2015_2020\",\n",
    "        \"kia_carnival_van_2021_\",\n",
    "        \"kia_k5_sedan_2010_2015\",\n",
    "        \"kia_k5_sedan_2020_\",\n",
    "        \"kia_k7_sedan_2016_2020\",\n",
    "        \"kia_mohave_suv_2020_\",\n",
    "        \"kia_morning_hatchback_2004_2010\",\n",
    "        \"kia_morning_hatchback_2011_2016\",\n",
    "        \"kia_ray_hatchback_2012_2017\",\n",
    "        \"kia_sorrento_suv_2015_2019\",\n",
    "        \"kia_sorrento_suv_2020_\",\n",
    "        \"kia_soul_suv_2014_2018\",\n",
    "        \"kia_sportage_suv_2016_2020\",\n",
    "        \"kia_stonic_suv_2017_2019\",\n",
    "        \"renault_sm3_sedan_2015_2018\",\n",
    "        \"renault_xm3_suv_2020_\",\n",
    "        \"ssangyong_korando_suv_2019_2020\",\n",
    "        \"ssangyong_tivoli_suv_2016_2020\",\n",
    "    ]\n",
    "\n",
    "\n",
    "CFG.__dict__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16fd60a5-24e2-4539-bfd0-1c374a641699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, transforms=None, num_classes: int = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def to_categorical(self, y, num_classes):\n",
    "        \"\"\"1-hot encodes a tensor\"\"\"\n",
    "        return np.eye(num_classes, dtype=\"uint8\")[y]\n",
    "\n",
    "    def get_class_weight(self):\n",
    "        return torch.Tensor(\n",
    "            compute_class_weight(\n",
    "                class_weight=\"balanced\", classes=np.unique(self.y), y=self.y\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.X[index]\n",
    "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)[\"image\"]\n",
    "\n",
    "        if self.y is None:  # if test\n",
    "            return image, img_path\n",
    "\n",
    "        # train or valid\n",
    "        label = self.y[index]\n",
    "        if self.num_classes is None:\n",
    "            return image, label, img_path\n",
    "        else:\n",
    "            return image, self.to_categorical(label, self.num_classes), img_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42a7a17f",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c01b4067-0669-44a9-bbbc-3065d8cb00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0,\n",
    "            rotate_limit=5,\n",
    "            border_mode=0,\n",
    "            value=(0, 0, 0),\n",
    "            p=0.9,\n",
    "        ),\n",
    "        A.GridDropout(),\n",
    "        A.Resize(456, 456),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ToGray(p=1),\n",
    "        A.GaussianBlur(blur_limit=(3, 21), p=1),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(456, 456),\n",
    "        A.ToGray(p=1),\n",
    "        A.GaussianBlur(blur_limit=(3, 21), p=1),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed83ca14",
   "metadata": {},
   "source": [
    "## Init dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a223cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "for i, cls in enumerate(range(CFG.num_classes)):\n",
    "    data_path_list = sorted(glob(f\"{CFG.data_path}/{cls}/*.png\"))\n",
    "    data_path_list = np.expand_dims(np.array(data_path_list), 1)\n",
    "\n",
    "    labels = np.ones(data_path_list.shape, dtype=np.uint8) * i\n",
    "\n",
    "    temp = np.concatenate([data_path_list, labels], axis=1)\n",
    "\n",
    "    data = temp if data is None else np.concatenate([data, temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3997adcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/dacon_cars/data/cars/0/syn_00024_2.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/data/dacon_cars/data/cars/0/syn_00053_1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/data/dacon_cars/data/cars/0/syn_00060_1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/data/dacon_cars/data/cars/0/syn_00061_1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/data/dacon_cars/data/cars/0/syn_00077_1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16995</th>\n",
       "      <td>/data/dacon_cars/data/cars/33/syn_06341_0.png</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16996</th>\n",
       "      <td>/data/dacon_cars/data/cars/33/syn_06347_0.png</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16997</th>\n",
       "      <td>/data/dacon_cars/data/cars/33/syn_06434_2.png</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16998</th>\n",
       "      <td>/data/dacon_cars/data/cars/33/syn_06437_0.png</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999</th>\n",
       "      <td>/data/dacon_cars/data/cars/33/syn_06471_0.png</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  label\n",
       "0       /data/dacon_cars/data/cars/0/syn_00024_2.png      0\n",
       "1       /data/dacon_cars/data/cars/0/syn_00053_1.png      0\n",
       "2       /data/dacon_cars/data/cars/0/syn_00060_1.png      0\n",
       "3       /data/dacon_cars/data/cars/0/syn_00061_1.png      0\n",
       "4       /data/dacon_cars/data/cars/0/syn_00077_1.png      0\n",
       "...                                              ...    ...\n",
       "16995  /data/dacon_cars/data/cars/33/syn_06341_0.png     33\n",
       "16996  /data/dacon_cars/data/cars/33/syn_06347_0.png     33\n",
       "16997  /data/dacon_cars/data/cars/33/syn_06434_2.png     33\n",
       "16998  /data/dacon_cars/data/cars/33/syn_06437_0.png     33\n",
       "16999  /data/dacon_cars/data/cars/33/syn_06471_0.png     33\n",
       "\n",
       "[17000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.columns = [\"path\", \"label\"]\n",
    "df = df.astype({\"path\": \"string\", \"label\": \"int\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e1f8c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 500\n",
      "1 : 500\n",
      "2 : 500\n",
      "3 : 500\n",
      "4 : 500\n",
      "5 : 500\n",
      "6 : 500\n",
      "7 : 500\n",
      "8 : 500\n",
      "9 : 500\n",
      "10 : 500\n",
      "11 : 500\n",
      "12 : 500\n",
      "13 : 500\n",
      "14 : 500\n",
      "15 : 500\n",
      "16 : 500\n",
      "17 : 500\n",
      "18 : 500\n",
      "19 : 500\n",
      "20 : 500\n",
      "21 : 500\n",
      "22 : 500\n",
      "23 : 500\n",
      "24 : 500\n",
      "25 : 500\n",
      "26 : 500\n",
      "27 : 500\n",
      "28 : 500\n",
      "29 : 500\n",
      "30 : 500\n",
      "31 : 500\n",
      "32 : 500\n",
      "33 : 500\n",
      "\n",
      "전체 : 17000\n"
     ]
    }
   ],
   "source": [
    "for i, cls in enumerate(range(CFG.num_classes)):\n",
    "    print(f'{cls} : {df[df[\"label\"] == i].shape[0]}')\n",
    "\n",
    "print(\"\")\n",
    "print(f\"전체 : {df.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee6967ed",
   "metadata": {},
   "source": [
    "## Train / Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4674bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "data_fold_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "checker = data_fold_splitter.get_n_splits(X=df, y=df[\"label\"])\n",
    "print(checker)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50ef1ec0",
   "metadata": {},
   "source": [
    "## FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3239d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        # print(self.gamma)\n",
    "        self.eps = eps\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = self.ce(input, target)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c11df848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "\n",
    "def save_pred(saved_path, path, y_true, y_pred, y_prob=None):\n",
    "    os.makedirs(saved_path)\n",
    "\n",
    "    df_data = [path, y_true, y_pred]\n",
    "    df_columns = [\"path\", \"y_true\", \"y_pred\"]\n",
    "\n",
    "    if y_prob != None:\n",
    "        df_data.append(y_prob)\n",
    "        df_columns.append(\"y_prob\")\n",
    "\n",
    "    df = pd.DataFrame(np.array(df_data).T)\n",
    "    df.columns = df_columns\n",
    "\n",
    "    df.to_csv(f\"{saved_path}/pred.csv\", index=False)\n",
    "\n",
    "    ### 임시 confusion_matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.set(rc={\"figure.figsize\": (21, 21)})\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "    _val_score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    plt.xlabel(f\"Pred / F1-score: {_val_score:.3f}\")\n",
    "    plt.ylabel(\"Real\")\n",
    "\n",
    "    classes_point = list(map(lambda x: x + 0.5, range(CFG.num_classes)))\n",
    "    classes = list(range(CFG.num_classes))\n",
    "    plt.xticks(classes_point, classes)\n",
    "    plt.yticks(classes_point, classes)\n",
    "    plt.savefig(f\"{saved_path}/c_matrix.jpg\")\n",
    "    plt.clf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "## Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1011ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.image_eda.v1 import tensor2im\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "\n",
    "def valid(model, criterion, data_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    epoch_paths = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # y_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (images, labels, paths) in enumerate(tqdm(data_loader)):\n",
    "            images = images.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "            probs = model(images)\n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            probs = probs.cpu().detach().numpy()\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "\n",
    "            preds = np.argmax(probs, 1).astype(np.uint8)\n",
    "            labels = np.argmax(labels, 1).astype(np.uint8)\n",
    "\n",
    "            preds = preds.flatten()\n",
    "            labels = labels.flatten()\n",
    "\n",
    "            y_pred += preds.tolist()\n",
    "            y_true += labels.tolist()\n",
    "            # y_probs += probs.tolist()\n",
    "            epoch_paths += paths\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_score = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_score\": val_score,\n",
    "        \"path\": epoch_paths,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "\n",
    "    epoch_paths = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch_index, (images, labels, paths) in enumerate(tqdm(data_loader)):\n",
    "        if batch_index % 10 == 0:\n",
    "            temp_img = images[0].detach().cpu()\n",
    "            temp_img = tensor2im(temp_img)\n",
    "\n",
    "            cv2.imwrite(f\"{CFG.root_path}/temp/training_img.jpg\", temp_img)\n",
    "\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        probs = model(images)\n",
    "        loss = criterion(probs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        probs = probs.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "\n",
    "        preds = np.argmax(probs, 1).astype(np.uint8)\n",
    "        labels = np.argmax(labels, 1).astype(np.uint8)\n",
    "\n",
    "        preds = preds.flatten()\n",
    "        labels = labels.flatten()\n",
    "\n",
    "        y_pred += preds.tolist()\n",
    "        y_true += labels.tolist()\n",
    "        epoch_paths += paths\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_loss)\n",
    "    train_score = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_score\": train_score,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcd63b0f",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0caf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes: int):\n",
    "    model = models.efficientnet_b5(weights=models.EfficientNet_B5_Weights.DEFAULT)\n",
    "    model.classifier = nn.Sequential(nn.Linear(2048, num_classes), nn.Sigmoid())\n",
    "\n",
    "    # model = timm.models.eva.eva02_base_patch14_448(pretrained=True)\n",
    "    # model.head = nn.Sequential(nn.Linear(768, num_classes), nn.Sigmoid())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# create_model(CFG.num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b230af0",
   "metadata": {},
   "source": [
    "## Snapshot Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d6e487f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/dacon_cars/outputs/CLASSICIATION_1/CLASSICIATION_1.ipynb'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "try:\n",
    "    import IPython\n",
    "\n",
    "    notebook_path = IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"]\n",
    "except:\n",
    "    notebook_path = f\"{os.getcwd()}/{CFG.job_id}.ipynb\"\n",
    "\n",
    "\n",
    "os.makedirs(CFG.outputs_path, exist_ok=True)\n",
    "shutil.copy(notebook_path, f\"{CFG.outputs_path}/{os.path.split(notebook_path)[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "005b7ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6358e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.random_seed.v1 import seed_everything, seed_worker\n",
    "\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86142d9a-68b7-4d04-8423-49d28025411d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_0 start\n",
      "loaded current_epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:14<00:00,  2.71it/s]\n",
      "100%|██████████| 213/213 [00:28<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, train_loss: 0.0006884728579419008, val_loss: 6.70782355711896e-05, train_score: 0.9983823529411765, val_score: 0.9997058823529412, train_elapsed_time: 314.1787226200104, val_elapsed_time: 28.450924396514893\n",
      "best epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:09<00:00,  2.74it/s]\n",
      "100%|██████████| 213/213 [00:25<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, train_loss: 0.0004753553890451953, val_loss: 5.5572672359639355e-05, train_score: 0.99875, val_score: 1.0, train_elapsed_time: 309.9650390148163, val_elapsed_time: 25.5843448638916\n",
      "best epoch: 7\n",
      "fold_1 start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:00<00:00,  2.83it/s]\n",
      "100%|██████████| 213/213 [00:28<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 0.12517743481432692, val_loss: 0.04941478586742576, train_score: 0.25816176470588237, val_score: 0.9997058823529412, train_elapsed_time: 300.4715631008148, val_elapsed_time: 28.43472456932068\n",
      "best epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, train_loss: 0.010777503996128764, val_loss: 0.002260611786074202, train_score: 0.9833088235294117, val_score: 0.9994117647058823, train_elapsed_time: 303.3582832813263, val_elapsed_time: 26.02751612663269\n",
      "best epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:27<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, train_loss: 0.002524190335192115, val_loss: 0.0004811164636892909, train_score: 0.9960294117647058, val_score: 1.0, train_elapsed_time: 303.8697609901428, val_elapsed_time: 27.973042726516724\n",
      "best epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, train_loss: 0.0014298275509260266, val_loss: 0.00023870302361224125, train_score: 0.9970588235294118, val_score: 1.0, train_elapsed_time: 303.4703974723816, val_elapsed_time: 26.081753730773926\n",
      "best epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.82it/s]\n",
      "100%|██████████| 213/213 [00:28<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, train_loss: 0.0008726457499399069, val_loss: 9.549545382281783e-05, train_score: 0.9982352941176471, val_score: 1.0, train_elapsed_time: 301.6479218006134, val_elapsed_time: 28.696011066436768\n",
      "best epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:25<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, train_loss: 0.0007446246126646121, val_loss: 0.00010032575796322761, train_score: 0.9981617647058824, val_score: 1.0, train_elapsed_time: 303.8369576931, val_elapsed_time: 25.907635927200317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.82it/s]\n",
      "100%|██████████| 213/213 [00:27<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, train_loss: 0.0006313960499777321, val_loss: 4.956885766685599e-05, train_score: 0.9981617647058824, val_score: 1.0, train_elapsed_time: 301.22431230545044, val_elapsed_time: 27.289607524871826\n",
      "best epoch: 7\n",
      "fold_2 start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.81it/s]\n",
      "100%|██████████| 213/213 [00:25<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 0.1203613060143064, val_loss: 0.03714618727809667, train_score: 0.31272058823529414, val_score: 0.9997058823529412, train_elapsed_time: 303.032217502594, val_elapsed_time: 25.20975923538208\n",
      "best epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.82it/s]\n",
      "100%|██████████| 213/213 [00:28<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, train_loss: 0.009227832500713275, val_loss: 0.0020429292773530587, train_score: 0.9875735294117647, val_score: 1.0, train_elapsed_time: 301.21175146102905, val_elapsed_time: 28.44638204574585\n",
      "best epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, train_loss: 0.0025297209869056723, val_loss: 0.00045790257284356533, train_score: 0.9960294117647058, val_score: 1.0, train_elapsed_time: 303.5868573188782, val_elapsed_time: 26.3781476020813\n",
      "best epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.82it/s]\n",
      "100%|██████████| 213/213 [00:27<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, train_loss: 0.001238666977322496, val_loss: 0.00020287072272619293, train_score: 0.9976470588235294, val_score: 1.0, train_elapsed_time: 301.4182856082916, val_elapsed_time: 27.70730423927307\n",
      "best epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, train_loss: 0.0008233180156265221, val_loss: 8.065566779092444e-05, train_score: 0.9983823529411765, val_score: 1.0, train_elapsed_time: 303.97234201431274, val_elapsed_time: 26.359101057052612\n",
      "best epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.82it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, train_loss: 0.0007598810555389398, val_loss: 5.5225207927752124e-05, train_score: 0.9980882352941176, val_score: 1.0, train_elapsed_time: 301.92103576660156, val_elapsed_time: 26.91825294494629\n",
      "best epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:25<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, train_loss: 0.000515572074257173, val_loss: 2.1625754071494944e-05, train_score: 0.9988235294117647, val_score: 1.0, train_elapsed_time: 303.97593355178833, val_elapsed_time: 25.869572639465332\n",
      "best epoch: 7\n",
      "fold_3 start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.82it/s]\n",
      "100%|██████████| 213/213 [00:28<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 0.12251809564583442, val_loss: 0.054013060939284, train_score: 0.28941176470588237, val_score: 0.9976470588235294, train_elapsed_time: 301.6556441783905, val_elapsed_time: 28.3510684967041\n",
      "best epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, train_loss: 0.010392061478924006, val_loss: 0.0017513860675108404, train_score: 0.9816911764705882, val_score: 1.0, train_elapsed_time: 303.9099633693695, val_elapsed_time: 26.447911739349365\n",
      "best epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.82it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, train_loss: 0.002701888598351027, val_loss: 0.0006841054148512931, train_score: 0.9947794117647059, val_score: 1.0, train_elapsed_time: 301.49399495124817, val_elapsed_time: 26.960981845855713\n",
      "best epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:02<00:00,  2.81it/s]\n",
      "100%|██████████| 213/213 [00:25<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, train_loss: 0.0013701537065550357, val_loss: 0.00025186814835618637, train_score: 0.9976470588235294, val_score: 1.0, train_elapsed_time: 302.8342547416687, val_elapsed_time: 25.574493646621704\n",
      "best epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.82it/s]\n",
      "100%|██████████| 213/213 [00:27<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, train_loss: 0.0008571536377798218, val_loss: 9.432732494274141e-05, train_score: 0.9983823529411765, val_score: 1.0, train_elapsed_time: 301.8681631088257, val_elapsed_time: 27.08397936820984\n",
      "best epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, train_loss: 0.0008135546620092902, val_loss: 7.429416224804812e-05, train_score: 0.9972794117647059, val_score: 1.0, train_elapsed_time: 303.59102606773376, val_elapsed_time: 26.03249478340149\n",
      "best epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.82it/s]\n",
      "100%|██████████| 213/213 [00:28<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, train_loss: 0.0005216345691330977, val_loss: 3.51127965872869e-05, train_score: 0.9986764705882353, val_score: 1.0, train_elapsed_time: 301.1960186958313, val_elapsed_time: 28.558929920196533\n",
      "best epoch: 7\n",
      "fold_4 start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 0.1225032271883067, val_loss: 0.041225841295131495, train_score: 0.28816176470588234, val_score: 0.9991176470588236, train_elapsed_time: 303.8900170326233, val_elapsed_time: 26.0790753364563\n",
      "best epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.81it/s]\n",
      "100%|██████████| 213/213 [00:28<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, train_loss: 0.009699620180763304, val_loss: 0.0018326457592050935, train_score: 0.9868382352941176, val_score: 1.0, train_elapsed_time: 303.0361657142639, val_elapsed_time: 28.636332988739014\n",
      "best epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:04<00:00,  2.79it/s]\n",
      "100%|██████████| 213/213 [00:25<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, train_loss: 0.0025008654036997435, val_loss: 0.0005517248713308639, train_score: 0.995735294117647, val_score: 1.0, train_elapsed_time: 304.29794454574585, val_elapsed_time: 25.78041934967041\n",
      "best epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.81it/s]\n",
      "100%|██████████| 213/213 [00:28<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, train_loss: 0.0012673867502770223, val_loss: 0.0005334519548614908, train_score: 0.9978676470588236, val_score: 1.0, train_elapsed_time: 301.98462986946106, val_elapsed_time: 28.124454498291016\n",
      "best epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:03<00:00,  2.80it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, train_loss: 0.0008360190353500109, val_loss: 0.00012295176566112787, train_score: 0.9983823529411765, val_score: 1.0, train_elapsed_time: 303.9181933403015, val_elapsed_time: 26.34901189804077\n",
      "best epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:04<00:00,  2.79it/s]\n",
      "100%|██████████| 213/213 [00:26<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, train_loss: 0.0008060440399362158, val_loss: 9.903386051801006e-05, train_score: 0.9973529411764706, val_score: 1.0, train_elapsed_time: 304.37965178489685, val_elapsed_time: 26.379647731781006\n",
      "best epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [05:01<00:00,  2.82it/s]\n",
      "100%|██████████| 213/213 [00:28<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, train_loss: 0.0005927190040315242, val_loss: 4.149974488150143e-05, train_score: 0.9983088235294117, val_score: 1.0, train_elapsed_time: 301.92953181266785, val_elapsed_time: 28.156135082244873\n",
      "best epoch: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2100x2100 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.recorder.v1 import Recorder\n",
    "from time import time\n",
    "\n",
    "\n",
    "for fold_index, (train_idx, valid_idx) in enumerate(\n",
    "    data_fold_splitter.split(X=df, y=df[\"label\"])\n",
    "):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[valid_idx]\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        train_df[\"path\"].values,\n",
    "        train_df[\"label\"].values,\n",
    "        train_transform,\n",
    "        CFG.num_classes,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.batch_size * 2,  #\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        worker_init_fn=seed_worker,\n",
    "    )\n",
    "    #\n",
    "    val_dataset = CustomDataset(\n",
    "        val_df[\"path\"].values, val_df[\"label\"].values, val_transform, CFG.num_classes\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.batch_size * 2,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        worker_init_fn=seed_worker,\n",
    "    )\n",
    "    model = create_model(CFG.num_classes)\n",
    "    model.to(device)\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=CFG.learning_late)\n",
    "    scheduler = None\n",
    "\n",
    "    recorder = Recorder(\n",
    "        f\"{CFG.recorder_dir}/fold_{fold_index}\", model, optimizer, scheduler\n",
    "    )\n",
    "    print(f\"fold_{fold_index} start\")\n",
    "    if recorder.load_checkpoint(device, \"checkpoint.pt\"):\n",
    "        print(f\"loaded current_epoch: {recorder.current_epoch}\")\n",
    "\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    best_val_loss = 100\n",
    "    for epoch_index in range(recorder.current_epoch, CFG.epoch):\n",
    "        seed_everything(epoch_index)\n",
    "\n",
    "        train_start_timestamp = time()\n",
    "        train_dict = train(model, criterion, optimizer, train_loader, device)\n",
    "        train_elapsed_time = time() - train_start_timestamp\n",
    "\n",
    "        val_start_timestamp = time()\n",
    "        val_dict = valid(model, criterion, val_loader, device)\n",
    "        val_elapsed_time = time() - val_start_timestamp\n",
    "\n",
    "        recorder.update_row_dict(\"epoch\", epoch_index + 1)\n",
    "\n",
    "        recorder.update_row_dict(\"train_loss\", train_dict[\"train_loss\"])\n",
    "        recorder.update_row_dict(\"val_loss\", val_dict[\"val_loss\"])\n",
    "\n",
    "        recorder.update_row_dict(\"train_score\", train_dict[\"train_score\"])\n",
    "        recorder.update_row_dict(\"val_score\", val_dict[\"val_score\"])\n",
    "\n",
    "        recorder.update_row_dict(\"train_elapsed_time\", train_elapsed_time)\n",
    "        recorder.update_row_dict(\"val_elapsed_time\", val_elapsed_time)\n",
    "        recorder.flush_row_dict(is_print=True)\n",
    "        recorder.save_line_plot([\"loss\"], [0, 0.1])\n",
    "\n",
    "        save_pred(\n",
    "            f\"{CFG.predict_dir}/fold_{fold_index}/{epoch_index}\",\n",
    "            val_dict[\"path\"],\n",
    "            val_dict[\"y_true\"],\n",
    "            val_dict[\"y_pred\"],\n",
    "        )\n",
    "\n",
    "        if recorder.is_best_score(val_dict[\"val_loss\"], \"min\"):\n",
    "            print(f\"best epoch: {epoch_index + 1}\")\n",
    "            recorder.save_checkpoint(epoch_index, \"best_model.pt\")\n",
    "\n",
    "        recorder.save_checkpoint(epoch_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22d0a58d",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80762fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(456, 456),\n",
    "        A.ToGray(p=1),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97c07821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5.5572672359639355e-05\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "\n",
    "for i in range(1):\n",
    "    model = create_model(CFG.num_classes)\n",
    "    model.to(device)\n",
    "    model.cuda()\n",
    "\n",
    "    check_point = torch.load(\n",
    "        f\"{CFG.recorder_dir}/fold_{i}/best_model.pt\",\n",
    "        map_location=device,\n",
    "    )\n",
    "    print(check_point[\"epoch\"])\n",
    "    print(check_point[\"best_score\"])\n",
    "    model.load_state_dict(check_point[\"model\"])\n",
    "    model.eval()\n",
    "    model_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "904aac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb73ac32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_list \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(glob(\u001b[39m\"\u001b[39m\u001b[39m/data/dacon_cars/data/test_cars/*.png\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(test_list[test_index])\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(test_index)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "test_list = sorted(glob(\"/data/dacon_cars/data/test_cars/*.png\"))\n",
    "print(test_list[test_index])\n",
    "print(test_index)\n",
    "img = cv2.cvtColor(cv2.imread(test_list[test_index]), cv2.COLOR_BGR2RGB)\n",
    "test_index += 1\n",
    "with torch.no_grad():\n",
    "    test_img = test_transform(image=img)[\"image\"]\n",
    "    test_img = torch.Tensor(test_img).to(device, dtype=torch.float)\n",
    "    test_img = torch.unsqueeze(test_img, 0)\n",
    "    probs = model_list[0](test_img)\n",
    "\n",
    "    probs = np.round(probs.cpu().detach().numpy(), 3)\n",
    "    top1 = np.max(probs)\n",
    "    label = np.argmax(probs)\n",
    "    print(probs)\n",
    "    print(top1)\n",
    "    print(label)\n",
    "    print(CFG.classes[label])\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img)\n",
    "    # f\"{test_path}/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80a080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
