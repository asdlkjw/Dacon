{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'groundingdino'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m box_convert\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msupervision\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msv\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnotebooks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mGroundingDINO\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroundingdino\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minference\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model, load_image, predict, annotate\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m     21\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/dacon_cars/notebooks/train/GroundingDINO/groundingdino/util/inference.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m box_convert\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbisect\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgroundingdino\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mT\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgroundingdino\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m build_model\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgroundingdino\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmisc\u001b[39;00m \u001b[39mimport\u001b[39;00m clean_state_dict\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'groundingdino'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from natsort import natsorted\n",
    "from attrdict import AttrDict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "import supervision as sv\n",
    "from notebooks.train.GroundingDINO.groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\n",
    "    \"/root/twkim/GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py\",\n",
    "    \"/root/twkim/groundingdino_swinb_cogcoor.pth\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_result(\n",
    "    source_h: int, source_w: int, boxes: torch.Tensor, logits: torch.Tensor\n",
    ") -> sv.Detections:\n",
    "    boxes = boxes * torch.Tensor([source_w, source_h, source_w, source_h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    confidence = logits.numpy()\n",
    "    return sv.Detections(xyxy=xyxy, confidence=confidence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image list & cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_LIST = natsorted(glob.glob(\"/root/twkim/test/*.png\"))\n",
    "TEXT_PROMPT = \"car\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok2\n"
     ]
    }
   ],
   "source": [
    "path = \"/root/dacon/cars/data/annotated\"\n",
    "crop = \"/root/dacon/cars/data/test_crop\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    print(\"ok1\")\n",
    "if not os.path.exists(crop):\n",
    "    os.makedirs(crop)\n",
    "    print(\"ok2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image load & inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_result(\n",
    "    source_h: int, source_w: int, boxes: torch.Tensor, logits: torch.Tensor\n",
    ") -> sv.Detections:\n",
    "    boxes = boxes * torch.Tensor([source_w, source_h, source_w, source_h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    confidence = logits.numpy()\n",
    "    return sv.Detections(xyxy=xyxy, confidence=confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/twkim\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bbox_calc:   1%|          | 26/3400 [00:09<19:49,  2.84it/s]"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(\"/root/twkim/sample_submission.csv\")\n",
    "\n",
    "for IMAGE_PATH in tqdm(IMAGE_LIST, desc=f\"bbox_calc\"):\n",
    "    image_source, image = load_image(IMAGE_PATH)\n",
    "    name = IMAGE_PATH.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD,\n",
    "    )\n",
    "\n",
    "    detections = post_process_result(\n",
    "        source_h=image_source.shape[0],\n",
    "        source_w=image_source.shape[1],\n",
    "        boxes=boxes,\n",
    "        logits=logits,\n",
    "    )\n",
    "\n",
    "    for idx, conf in enumerate(detections.confidence):\n",
    "        crop_image = image_source[\n",
    "            int(detections.xyxy[idx][1]) : int(detections.xyxy[idx][3]),\n",
    "            int(detections.xyxy[idx][0]) : int(detections.xyxy[idx][2]),\n",
    "        ]\n",
    "\n",
    "        # with open(crop + f\"/{name}_{idx}_{int(conf*100)}.pkl\", \"wb\") as f:\n",
    "        #     pickle.dump(crop_image, f)\n",
    "        # np.save(crop + f\"/{name}_{idx}_{int(conf*100)}.npy\", crop_image)\n",
    "\n",
    "        x1, y1, x2, y2 = detections.xyxy[idx]\n",
    "        results.loc[len(results)] = {\n",
    "            \"file_name\": f\"{name}.png\",\n",
    "            \"class_id\": np.nan,\n",
    "            \"confidence\": conf,\n",
    "            \"point1_x\": x1,\n",
    "            \"point1_y\": y1,\n",
    "            \"point2_x\": x2,\n",
    "            \"point2_y\": y1,\n",
    "            \"point3_x\": x2,\n",
    "            \"point3_y\": y2,\n",
    "            \"point4_x\": x1,\n",
    "            \"point4_y\": y2,\n",
    "        }\n",
    "\n",
    "    # annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    # cv2.imwrite(path + f\"/{name}_annotated_image.png\", annotated_frame)\n",
    "\n",
    "    # 결과를 CSV 파일로 저장\n",
    "    results.to_csv(\"./bbox_submit_npy_재현.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pkl\n",
    "with open(\"/root/dacon/cars/data/crop/070638309_90.pkl\", \"rb\") as f:\n",
    "    load_image = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate image with detections\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "labels = [\n",
    "    f\"{phrases[0]} {confidence:0.2f}\" \n",
    "    for _, _, confidence, _, _ \n",
    "    in detections]\n",
    "annotated_frame = box_annotator.annotate(scene=image_source.copy(), detections=detections, labels=labels)\n",
    "\n",
    "%matplotlib inline\n",
    "sv.plot_image(annotated_frame, (16, 16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
